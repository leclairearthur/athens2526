{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f17ceb",
   "metadata": {},
   "source": [
    "# Practical Work 4: Introduction to Gradient Descent with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79f28c",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68483d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea05b0",
   "metadata": {},
   "source": [
    "We will find the minimum of the function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined by\n",
    "$$ f(x) = f(x_0,x_1) = x_0^2 + x_1^2 - \\sin(2x_0)x_1 \\quad .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb317301",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr,nc = 256,256\n",
    "a = 3\n",
    "extent = ((-a-0.5/nc, a-0.5/nc, -a-0.5/nr, a-0.5/nr))\n",
    "xs = np.linspace(a, -a, nr)\n",
    "ys = np.linspace(-a, a, nc)\n",
    "xm, ym = np.meshgrid(xs, ys, indexing='ij')\n",
    "xm = xm.T\n",
    "ym = ym.T\n",
    "\n",
    "y = xm**2 + ym**2 - np.sin(2*xm)*ym\n",
    "\n",
    "# other choices (just for fun):\n",
    "#   y = np.sqrt(1+xm**2 + ym**2 - np.sin(xm*2)*ym)\n",
    "#   y = 2*ym**2-np.cos(xm*3)*ym + 2*xm**2 \n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(y,cmap = 'gray', extent=extent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d559c",
   "metadata": {},
   "source": [
    "**QUESTION :** \n",
    "\n",
    "- Define the function $f$ in Pytorch: `f`should take a torch.tensor `x` of shape $(2)$ as input and should output $f(x)$.\n",
    "- Test the function $f$ by computing $f(x)$ for $x=(0,1)$.\n",
    "- Compute its gradient at point $x=(0,1)$ with Pytorch. \n",
    "- Can you check the obtained values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # ...\n",
    "\n",
    "# ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5c153",
   "metadata": {},
   "source": [
    "**QUESTION :** Look at the following cell, which implements gradient descent (with fixed step size $\\tau$).\n",
    "\n",
    "Try to understand each line of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = xm**2 + ym**2 - np.sin(xm*2)*ym   # copy the formula to display function values in background\n",
    "\n",
    "x0 = np.array([-1.5,3.])\n",
    "x = torch.tensor(x0, requires_grad=True)\n",
    "\n",
    "tau = 0.1\n",
    "N = 1000\n",
    "xd = np.zeros((N,2))\n",
    "\n",
    "fxlist = []\n",
    "\n",
    "for n in range(N):\n",
    "    fx = f(x)\n",
    "    fx.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= tau*x.grad\n",
    "    x.grad.zero_()\n",
    "    xd[n,:] = x.detach().cpu()\n",
    "    fxlist.append(fx.item())\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(y,cmap = 'gray', extent=extent)\n",
    "plt.scatter(x0[0], x0[1],c='red',alpha=.5)\n",
    "plt.scatter(xd[:, 0], xd[:,1],c='deepskyblue',alpha=.5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(fxlist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe905499",
   "metadata": {},
   "source": [
    "**QUESTION:** Compare by doing gradient descent with the Pytorch routine ``torch.optim.SGD`` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6209363",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x0, requires_grad=True)\n",
    "optim = torch.optim.SGD([x], lr=tau)\n",
    "losslist = []\n",
    "for it in range(N):\n",
    "    loss = f(x)\n",
    "    losslist.append(loss.item())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
